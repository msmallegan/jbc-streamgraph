---
title: "03_jbc_niche_streamgraph"
output: html_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(stringsAsFactors = FALSE)
source("util/util.R")
source("util/eutilities.R")
source("util/.ncbi_api_key.R")

# devtools::install_github("hrbrmstr/streamgraph")
library(streamgraph)

library(rentrez)
library(tidyverse)
library(rslurm)
library(data.table)
```

## Retrieve PubMed entries for JBC niche terms

This ended up being a pretty large cluster job that took a few days to run. This could still be optimized, but it is primarily limited in query speed by NCBI's traffic limits. Regardless, I believe I've now collected all the data here using the `retrieve_pubmed.R` script called by `retreive_jbc_niche_articles.sh`.

```{r}
niche_terms <- read.csv("results/jbc_characteristic_terms.csv")
years <- 2005:2019

# Since I saved each batch in the data/ directory 
# using the index of the year and the index of the 
# term, there should be files corresponding to years 1-15
# and terms 1-100. 
# I'll now just check to see that all these files have been retrieved.
niche_files <- data.frame("fl" = list.files("data", full.names = T)) %>%
  filter(grepl("jbc_niche", fl)) %>%
  mutate(i = str_extract(fl, "\\d+") %>% as.numeric(),
         j = str_extract(fl, "\\d+(?=\\.)") %>% as.numeric()) %>%
  arrange(i, j)
# Now I want to make sure there is one of each. 
# Make sure all years are there
all(1:15 %in% niche_files$i)
# Now let's see how many terms per year. 
all_j <- 1:100
ns <- niche_files %>% group_by(i) %>%
  summarize(count = n(),
            missing_terms = all_j[!(all_j %in% j)] %>% paste(collapse = ";"))
ns
# Okay, it appears that a few terms are missing from each year...
# In particular, terms 7 and 30 are missing from all years. 
niche_terms[30,"MH"]
niche_terms[7,"MH"]
# And terms 
# 32;52;68 are missing from year 2 (2007)
niche_terms[c(32,52,68),"MH"]
# It's possible that terms 7 and 30 contained too many results to be returned
# properly with our search strategy. 
# Let's fill them in here.
# It's possible the other terms were just not present in that 2007. 
# To see if that's the case we can plot their occurence in time. 
# and see if it's low enough that feasibly 2007 didn't see any Proteolysis publications.
# TODO: check 32,52, 68 for year 2007

```

#### Retrieving mising terms

So there are a few that slipped through the cracks of the original retrieval method. Let's try to grab them now and see what went wrong.

```{r}
# Term 90 might have some issues as well...
niche_papers <- read_csv(paste0("data/", niche_files[which(niche_files$j == 90 & niche_files$i == 5),"fl"]))
as.character(niche_papers[1,"MH"])

# Okay, so it seems that "Mutation, Missense" was still picked up even though
# there was an asterisk in front of the query term "*Mutation, Missense"
# I believe if I remember correctly that the asterisk means that it was 
# the major focus of the article. I should maybe go back and scrub that
# from the terms before calculating TF-IDF and doing the clustering
# TODO: see how the results change when removing the asterisks and 
# potentially also the subheadings ("/[term]").
# For now though, I'll forge ahead.

```

To retrieve results for terms 7 and 30, I'll remove the subheadings from the terms
before executing the search. This will work for now, but to be consistent in the
analysis, the subheadings should be treated uniformly in the TF-IDF clustering and 
in the data retrieval here.

```{r}
# Okay, I'm going to re-org this a bit and do it in a cleaner way using rslurm.
pars <- data.frame(year = years)
pars$niche_terms <- rep(list(niche_terms$MH), times = length(years))

# So now things will be returned as R objects. 
# TODO: will have to integrate these results 
# or just re-run everything.. that will have to happen
# eventually. 
sopt <- list(time = '1:00:00')
sjob <- slurm_apply(retrieve_pubmed, pars, jobname = 'retrieve_missing',
                    add_objects = c("make_query", "api_key", "medline_parser", 
                                    "assemble_query_w_subheadings"),
                    nodes = 1, cpus_per_node = 5, submit = FALSE,
                    slurm_options = sopt)
# Nifty little bit of code here.
# system("source /etc/profile; cd _rslurm_retrieve_missing; sbatch submit.sh")

res <- get_slurm_out(sjob, outtype = 'raw', ncores = 6)

res_df <- bind_rows(res)

# cleanup_files(sjob)
```


```{r notes}
# The major issue with terms 7 and 30 are that they have subheadings and that
# is weirdly resulting in zero results. "Peptide Fragments" (Term 29)
# also has subheadings, but there are apparently results returned by that query.
# TODO: Check term 29 against the results searching pubmed with and without subheadings. 
# TODO: Whoops it appears that entrez_fetch may have had a default return of 10,000 articles
# anything that maxed out at 10k probably has more articles to retrive. bummer.

# pdat is the date of print publication
# That does seem like what we want, phew.

# TODO: Maybe. Test the difference between splitting up the subheadings and not
# Also, why was it that these terms showed up in the first place if they don't 
# capture anything with this query?
# It seems like it may not make a difference, but I do want to get it right.
# TODO: rerun the whole thing with the updated code including quotes on the query.
```


#### Gather collected data

```{r}
# okay, while all that junk is running, I can just gather the data I do have
# and put it in the format to make the streamgraph.
niche_files$year <- years[niche_files$i]
niche_files$term <- niche_terms$MH[niche_files$j]

# Hmm, this may take a while. Will probably save this as an R object. 
# and copy to my home directory as well. 
niche_papers <- do.call(rbind, lapply(1:nrow(niche_files), function(x) {
  dat <- fread(niche_files$fl[x])
  dat$term <- niche_files$term[x]
  dat$year <- niche_files$year[x]
  return(dat)
}))

niche_papers$PMID <- as.character(niche_papers$PMID)
# Since we did a part of the run previously we will want to merge
niche_papers <- bind_rows(niche_papers, res_df)

# save(niche_papers, file = "data/niche_papers.RData")
# save(niche_papers, file = "~/jbc-streamgraph/data/niche_papers.RData")

```


#### Compile into per year and journal

Okay, so to create the streamgraph we want articles per journal per year.

```{r}
# Let' count per journal and year]
niche_journal_summary <- niche_papers %>% 
  group_by(TA, year) %>%
  summarize(paper_count = length(unique(PMID)))
write.csv(niche_journal_summary, "results/jbc_niche_year_counts_per_journal.csv", row.names = FALSE)

# Okay let's put all the small journals into an other category
journal_totals <- niche_journal_summary %>% group_by(TA) %>%
  summarize(mean = mean(paper_count),
            min = min(paper_count),
            max = max(paper_count))

big_journals <- journal_totals %>% filter(mean > 400)
small_journals <- journal_totals %>% filter(mean <= 400)

other_counts <- niche_journal_summary %>% 
  filter(TA %in% small_journals$TA) %>%
  group_by(year) %>% 
  summarize(paper_count = sum(paper_count)) %>%
  mutate(TA = "Other") %>%
  select(TA, year, paper_count)
big_counts <- niche_journal_summary %>%
  filter(TA %in% big_journals$TA) 

paper_counts <- bind_rows(big_counts, other_counts)


```


#### Streamgraph

Okay, fingers crossed.

```{r}

streamgraph(paper_counts %>% filter(TA != "Other"), key = "TA", value = "paper_count", date = "year",
            width=1000,height=600, interactive=TRUE) %>%
  sg_axis_x(1, "Year") %>% sg_fill_brewer(palette = "Spectral") 
```




